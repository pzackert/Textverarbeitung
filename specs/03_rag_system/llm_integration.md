# LLM Integration Specification

## 1. Provider Abstraction
The system must support multiple local LLM providers via a unified interface `LLMClient`.

### Supported Providers
1.  **Ollama** (Primary)
    - Protocol: HTTP API (`http://localhost:11434/api/generate`)
    - Models: `qwen2.5:7b`, `qwen2.5:0.5b`
2.  **LM Studio** (Secondary)
    - Protocol: OpenAI-compatible API (`http://localhost:1234/v1`)
    - Models: Any loaded model

## 2. Model Configuration
Configuration managed in `config.yaml`:

```yaml
llm:
  provider: "ollama" # or "lm_studio"
  model_name: "qwen2.5:7b"
  temperature: 0.1 # Low temperature for factual RAG
  max_tokens: 2048
  top_p: 0.9
  timeout: 60 # seconds
```

## 3. Prompt Engineering
### System Prompt Template
```text
Du bist ein intelligenter Assistent für die IFB Hamburg (Investitions- und Förderbank).
Deine Aufgabe ist es, Fragen basierend auf den bereitgestellten Kontext-Dokumenten zu beantworten.

REGELN:
1. Antworte NUR basierend auf dem untenstehenden Kontext.
2. Wenn die Antwort nicht im Kontext steht, sage "Ich habe dazu keine Informationen in den Dokumenten gefunden."
3. Erfinde keine Fakten (Halluzination vermeiden).
4. Zitiere die Quelle (Dateiname), wenn möglich.
5. Antworte immer auf Deutsch.

KONTEXT:
{context}

FRAGE:
{question}
```

## 4. Response Parsing
- **Raw Text**: Extract `response` field from JSON.
- **Citation Extraction**: (Optional) Parse "Source: ..." references if generated by LLM.
- **Structured Output**: For Criteria Engine (Phase 4), force JSON output mode.

## 5. Error Handling
- **Connection Error**: Retry 3 times with exponential backoff.
- **Timeout**: Fail gracefully with "LLM did not respond in time."
- **Context Limit Exceeded**: Truncate context before sending (handled in Retrieval Strategy).
