# Criteria Engine - Specification

## Goal
Implement a deterministic evaluation engine that uses LLMs to check funding applications against a defined catalog of criteria, utilizing the **TOON (Token-Optimized Object Notation)** format for maximum efficiency.

## Requirements

### Functional
- [ ] **TOON Parsing**: Parse `.toon` criteria catalogs into Python objects.
- [ ] **Evaluation Logic**:
  - **Check (Knockout)**: Boolean pass/fail evaluation (e.g., "Location in Hamburg?").
  - **Score (Weighted)**: 1-5 scale evaluation (e.g., "Degree of Innovation").
- [ ] **LLM Reasoning**: Every evaluation must include a text justification generated by the LLM.
- [ ] **Context Integration**: Inject relevant RAG chunks into the prompt for each criterion.
- [ ] **Result Generation**: Output results in TOON format to save tokens during synthesis.
- [ ] **Aggregation**: Calculate weighted totals and determine overall "Fundable" status.

### Non-Functional
- [ ] **Token Efficiency**: TOON format must use >40% fewer tokens than equivalent JSON.
- [ ] **Determinism**: Set LLM temperature to 0.1 for consistent evaluations.
- [ ] **Traceability**: Log the exact prompt and response for every criterion check.

## Input/Output Definitions

### TOON Format Definition
TOON is a compact, line-based format optimized for LLMs.
- **Headers**: `key[count,]{columns}:`
- **Rows**: Comma-separated values.
- **Arrays**: `[item1, item2]`

**Example Catalog (`data/criteria/ifb_profi.toon`):**
```toon
# IFB PROFI Criteria Catalog
meta{version,program}:
1.0,"PROFI Standard"

criteria[3,]{id,type,weight,title,prompt}:
c1,check,1.0,"Hamburg Base","Is the company based in Hamburg?"
c2,score,0.4,"Innovation","Rate the technological innovation (1-5)."
c3,score,0.6,"Market","Rate the market potential (1-5)."
```

### Engine Interface (`src/criteria/engine.py`)
```python
class CriteriaEngine:
    async def evaluate_project(
        self, 
        project_id: str, 
        catalog_path: Path
    ) -> EvaluationResult:
        """Runs the full evaluation pipeline."""
        ...

    async def _evaluate_criterion(
        self, 
        criterion: Criterion, 
        context: str
    ) -> CriterionResult:
        """Evaluates a single criterion using LLM."""
        ...
```

### Output TOON Example
```toon
results[3,]{id,result,score,reasoning}:
c1,pass,null,"Address in Hamburg confirmed (Page 2)."
c2,null,4,"High innovation, uses novel AI approach."
c3,null,3,"Market is crowded but growing."

summary{total_score,status}:
3.4,"REVIEW_REQUIRED"
```

## Test Cases

| ID | Name | Description | Expected Result |
|----|------|-------------|-----------------|
| TC-CRI-01 | **TOON Parsing** | Parse valid `.toon` file | Returns correct Python list of objects |
| TC-CRI-02 | **Knockout Fail** | Evaluate project with wrong location | Result: `fail`, Status: `REJECTED` |
| TC-CRI-03 | **Scoring Logic** | Calculate weighted score | (4*0.4 + 3*0.6) = 3.4 |
| TC-CRI-04 | **Token Efficiency** | Compare TOON vs JSON output | TOON token count < 60% of JSON |
| TC-CRI-05 | **LLM Reasoning** | Check output for reasoning text | "reasoning" field is not empty |
| TC-CRI-06 | **Missing Context** | Evaluate with empty RAG context | Returns "Insufficient Information" |

## Success Criteria
- [ ] TOON parser correctly handles arrays and headers.
- [ ] Token savings target (40%) is met.
- [ ] Evaluation logic correctly aggregates scores.
- [ ] LLM prompts are effective (prompt engineering verified).

## Dependencies
- `src/ollama` (LLM Client)
- `src/rag` (Context Provider)
- `parsimonious` or `lark` (Optional, for strict TOON parsing if regex fails)

## Files to Create
- `src/criteria/toon.py` (Parser/Serializer)
- `src/criteria/models.py`
- `src/criteria/engine.py`
- `src/criteria/prompts.py`
- `data/criteria/ifb_profi.toon`
- `tests/test_criteria/test_toon.py`
- `tests/test_criteria/test_engine.py`
