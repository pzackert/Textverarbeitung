# Dokumentenparsing
## IFB PROFI - Automatisierte Antragspr√ºfung

**Version:** 2.0  
**Stand:** 10. November 2025

---

## üéØ GRUNDLEGENDES ZIEL

Wir bauen ein optimales RAG-System (Retrieval-Augmented Generation), das Dokumente der IFB Hamburg intelligent verarbeitet. Das System extrahiert den **kompletten Volltext** aus allen relevanten Dokumenten - **ohne k√ºnstliche Token-Beschr√§nkungen**. Jedes Dokument wird vollst√§ndig erfasst und in ChromaDB als Vektor-Datenbank gespeichert, damit sp√§ter pr√§zise Suchanfragen m√∂glich sind.

---

## üìÑ UNTERST√úTZTE DOKUMENTFORMATE

Das System unterst√ºtzt initial die drei wichtigsten Formate:

### PDF-Dokumente (*.pdf)
F√∂rderrichtlinien, offizielle Bescheide und Projektskizzen. Hier extrahieren wir den kompletten Text inklusive Strukturinformationen wie √úberschriften und Paragraphen.

**Verwendung:** Projektskizze, Projektantrag, F√∂rderrichtlinien

**Features:**
- Vollst√§ndige Textextraktion
- Strukturerkennung (√úberschriften, Abs√§tze, Listen)
- Tabellen-Extraktion
- Metadaten (Autor, Erstellungsdatum, Version)

### Word-Dokumente (*.docx)
Projektantr√§ge und ausgef√ºllte Formulare. Der Parser erkennt Formularfelder, extrahiert deren Inhalte strukturiert und beh√§lt die Dokumentenhierarchie bei.

**Verwendung:** Projektskizze, Projektantrag

**Features:**
- Volltext mit Formatierung
- Formularfeld-Erkennung (Key-Value-Paare)
- Tabellen-Extraktion
- Dokumentenhierarchie (Kapitel, Unterkapitel)

### Excel-Dateien (*.xlsx, *.xls)
Bewertungstabellen, Checklisten und strukturierte Daten. Tabellen werden intelligent in Text umgewandelt, wobei Spalten√ºberschriften und Zellinhalte sinnvoll verkn√ºpft werden.

**Verwendung:** Checklisten, Bewertungstabellen, Projektkalkulation

**Features:**
- Strukturierte Datenextraktion
- Intelligente Tabellen-zu-Text-Konvertierung
- Spalten√ºberschriften-Verkn√ºpfung
- Multi-Sheet-Support

### OCR-Funktionalit√§t (Zuk√ºnftig)
**OCR-Funktionalit√§t** wird als zuk√ºnftige Erweiterung vorbereitet. Der Code wird so strukturiert, dass OCR-Module sp√§ter nahtlos integriert werden k√∂nnen, um auch gescannte PDFs und Bilddateien zu verarbeiten.

**Geplante Features:**
- Tesseract OCR Integration
- Bildqualit√§ts-Optimierung
- Layout-Analyse f√ºr strukturierte Extraktion

---

## üóÇÔ∏è DOKUMENT-TYPEN

### 1. Projektskizze
**Umfang:** 2-3 Seiten  
**Format:** PDF oder DOCX

**Inhalt:**
- Ansprechpartner (Liste)
- Unternehmensbeschreibung
- Technologischer L√∂sungsansatz
- Marktpotenzial und Vermarktung
- Projektumfang

**Parsing-Strategie:** Volltext-Extraktion mit Abschnittserkennung

### 2. Projektantrag (Formular)
**Umfang:** Mehrseitiges Formular + Anh√§nge  
**Format:** PDF oder DOCX

**Pflichtdokumente:**
- Projektbeschreibung (Formularfelder)
- Projektkalkulation (Excel)
- KMU-Erkl√§rung (PDF)
- Jahresabschl√ºsse (2x PDF)
- Handelsregisterauszug (PDF)
- Finanz- und Arbeitsplatz√ºbersicht (Excel)

**Optional:**
- Lebensl√§ufe (PDF)
- Letters of Intent (PDF)

**Parsing-Strategie:** Formularfelder als Key-Value-Paare + Volltext

---

## üîÑ EXTRAKTIONS-STRATEGIE

Bei jedem Dokument extrahieren wir **drei Ebenen von Information**:

### 1. Volltext
Der komplette Inhalt **ohne Verluste**. Wir setzen keine k√ºnstlichen Grenzen bei der Textl√§nge. Wenn ein F√∂rderrichtlinien-Dokument 200 Seiten hat, erfassen wir alle 200 Seiten.

**Ausgabe:**
```json
{
  "volltext": "Kompletter Dokumententext ohne K√ºrzungen..."
}
```

### 2. Strukturdaten
√úberschriften, Kapitelnummern, Paragraphen, Listen und Tabellen werden als solche erkannt und markiert. Diese Struktur hilft sp√§ter bei der gezielten Suche.

**Ausgabe:**
```json
{
  "struktur": {
    "kapitel": [
      {
        "nummer": "1",
        "titel": "Einleitung",
        "abs√§tze": ["Absatz 1...", "Absatz 2..."]
      }
    ],
    "tabellen": [
      {
        "name": "Kosten√ºbersicht",
        "rows": [...]
      }
    ]
  }
}
```

### 3. Metadaten
Dokumenttyp, Erstellungsdatum, Version, Autor und Programmzugeh√∂rigkeit (PROFI, etc.) werden separat gespeichert f√ºr effiziente Filterung.

**Ausgabe:**
```json
{
  "metadaten": {
    "dokumenttyp": "projektskizze",
    "erstellt_am": "2025-03-15",
    "version": "1.2",
    "autor": "Mustermann GmbH",
    "programm": "PROFI Standard",
    "seiten": 3,
    "dateigr√∂√üe": "2.4 MB"
  }
}
```

---

## üèóÔ∏è MODULARE ARCHITEKTUR

Der Code wird strikt modular aufgebaut mit klarer Trennung der Verantwortlichkeiten:

```
DocumentProcessor (Hauptkoordinator)
‚îú‚îÄ‚îÄ FormatDetector (erkennt Dateityp)
‚îú‚îÄ‚îÄ ParserFactory (w√§hlt richtigen Parser)
‚îú‚îÄ‚îÄ Parser-Module
‚îÇ   ‚îú‚îÄ‚îÄ PDFParser
‚îÇ   ‚îú‚îÄ‚îÄ WordParser  
‚îÇ   ‚îú‚îÄ‚îÄ ExcelParser
‚îÇ   ‚îî‚îÄ‚îÄ BaseParser (Fallback)
‚îú‚îÄ‚îÄ TextProcessor (Bereinigung, Normalisierung)
‚îú‚îÄ‚îÄ ChunkGenerator (intelligente Textaufteilung)
‚îî‚îÄ‚îÄ VectorStore (ChromaDB-Integration)
```

### Parser-Schnittstelle

Jeder Parser implementiert dieselbe Schnittstelle mit drei Kernmethoden:

```python
class BaseParser(ABC):
    """Basis-Interface f√ºr alle Parser."""
    
    @abstractmethod
    def extract_text(self, file_path: Path) -> str:
        """Holt den Rohtext."""
        pass
    
    @abstractmethod
    def extract_structure(self, file_path: Path) -> dict:
        """Erkennt Dokumentstruktur."""
        pass
    
    @abstractmethod
    def extract_metadata(self, file_path: Path) -> dict:
        """Sammelt Metainformationen."""
        pass
```

### Parser-Factory

```python
class ParserFactory:
    """W√§hlt den richtigen Parser basierend auf Dateityp."""
    
    _parsers = {
        ".pdf": PDFParser,
        ".docx": WordParser,
        ".xlsx": ExcelParser,
        ".xls": ExcelParser
    }
    
    @classmethod
    def get_parser(cls, file_path: Path) -> BaseParser:
        """Gibt passenden Parser zur√ºck."""
        suffix = file_path.suffix.lower()
        parser_class = cls._parsers.get(suffix, BaseParser)
        return parser_class()
    
    @classmethod
    def register_parser(cls, extension: str, parser_class: type):
        """Registriert neuen Parser f√ºr Erweiterung."""
        cls._parsers[extension] = parser_class
```

---

## üîß ERWEITERBARKEIT ALS KERNPRINZIP

Neue Formate werden durch simple Erg√§nzung unterst√ºtzt:

### Schritte zur Erweiterung:

1. **Neuen Parser erstellen** (z.B. `RTFParser`)
   ```python
   class RTFParser(BaseParser):
       def extract_text(self, file_path: Path) -> str:
           # RTF-spezifische Logik
           pass
   ```

2. **Von BaseParser ableiten**
   - Implementiere alle drei Kernmethoden
   - Verwende bew√§hrte Libraries

3. **In ParserFactory registrieren**
   ```python
   ParserFactory.register_parser(".rtf", RTFParser)
   ```

**Der bestehende Code muss nicht modifiziert werden.** Das System erkennt automatisch das neue Format und nutzt den entsprechenden Parser.

---

## üß© INTELLIGENTES CHUNKING F√úR CHROMADB

Dokumente werden intelligent in Chunks aufgeteilt f√ºr optimales RAG-Retrieval:

### Chunking-Strategie

#### 1. Semantische Grenzen
Wir trennen an Abs√§tzen und Kapiteln, **nicht mitten im Satz**.

```python
chunk_size = 1000  # Zeichen, keine Token-Limit!
chunk_overlap = 200  # 20% √úberlappung
separators = ["\n\n", "\n", ". ", " ", ""]
```

#### 2. Kontexterhaltung
Chunks √ºberlappen sich um 20%, damit Zusammenh√§nge nicht verloren gehen.

**Beispiel:**
```
Chunk 1: "...Ende von Absatz 1. Beginn Absatz 2..."
Chunk 2: "Beginn Absatz 2... Ende Absatz 2. Beginn Absatz 3..."
```

#### 3. Flexible Gr√∂√üe
- Kurze Abschnitte bleiben zusammen
- Lange werden sinnvoll geteilt
- Tabellen bleiben vollst√§ndig zusammen

#### 4. Metadaten-Vererbung
Jeder Chunk wei√ü, aus welchem Dokument, Kapitel und Abschnitt er stammt.

```json
{
  "chunk_id": "chunk_001",
  "text": "Chunk-Inhalt...",
  "metadata": {
    "dokument_id": "projektskizze_001",
    "dokumenttyp": "projektskizze",
    "kapitel": "1. Einleitung",
    "seite": 1,
    "chunk_index": 0
  }
}
```

---

## üéØ SPEZIALBEHANDLUNG F√úR IFB-DOKUMENTE

### Projektskizzen
**Formularfelder** werden als Key-Value-Paare extrahiert. Die Feldbezeichnung wird mit dem Inhalt verkn√ºpft f√ºr pr√§zise Suche.

**Beispiel:**
```json
{
  "formularfelder": {
    "Projektname": "Entwicklung einer KI-gest√ºtzten Verpackungsanlage",
    "Antragsteller": "Mustermann GmbH",
    "Projektlaufzeit": "24 Monate",
    "Gesamtkosten": "450.000 EUR"
  }
}
```

### Projektantr√§ge
**Mehrseitige Antr√§ge** behalten ihre Abschnittsstruktur. Anh√§nge werden erkannt und verlinkt.

**Beispiel:**
```json
{
  "abschnitte": [
    {
      "nummer": "A",
      "titel": "Projektbeschreibung",
      "inhalt": "..."
    },
    {
      "nummer": "B",
      "titel": "Projektkalkulation",
      "anhang": "kalkulation.xlsx"
    }
  ]
}
```

### Checklisten
**Kriterien und Bewertungen** werden strukturiert erfasst. Checkboxen werden in maschinenlesbare Ja/Nein-Werte √ºbersetzt.

**Beispiel:**
```json
{
  "kriterien": [
    {
      "id": "K001",
      "beschreibung": "Betriebsst√§tte in Hamburg",
      "status": true,
      "bewertung": "erf√ºllt"
    }
  ]
}
```

---

## ‚úÖ QUALIT√ÑTSSICHERUNG

### UTF-8 √ºberall
Deutsche Umlaute und Sonderzeichen werden korrekt behandelt.

```python
with open(file_path, "r", encoding="utf-8") as f:
    content = f.read()
```

### Fehlertoleranz
Besch√§digte Dokumente f√ºhren nicht zum Absturz. Der Parser extrahiert, was m√∂glich ist, und loggt Probleme.

```python
try:
    text = parser.extract_text(file_path)
except ParsingError as e:
    logger.warning(f"Parsing-Fehler in {file_path}: {e}")
    text = parser.extract_partial_text(file_path)
```

### Vollst√§ndigkeitspr√ºfung
Nach dem Parsing wird verifiziert, dass kein Inhalt verloren ging.

```python
def verify_completeness(original_file: Path, extracted_text: str) -> bool:
    """Pr√ºft, ob Extraktion vollst√§ndig ist."""
    original_size = original_file.stat().st_size
    extracted_size = len(extracted_text.encode('utf-8'))
    
    # Warnung bei gro√üer Diskrepanz
    if extracted_size < original_size * 0.5:
        logger.warning(f"Nur {extracted_size}/{original_size} Bytes extrahiert")
        return False
    
    return True
```

---

## üõ†Ô∏è TECHNISCHE UMSETZUNG

### Verwendete Libraries

**PyMuPDF (fitz)** f√ºr PDF-Verarbeitung
- Schnell und zuverl√§ssig
- Vollst√§ndige Textextraktion
- Tabellen-Support
```bash
pip install PyMuPDF
```

**python-docx** f√ºr Word-Dokumente
- Native DOCX-Unterst√ºtzung
- Formularfeld-Erkennung
- Tabellenextraktion
```bash
pip install python-docx
```

**openpyxl** f√ºr Excel-Files
- Vollst√§ndige Format-Unterst√ºtzung
- Multi-Sheet-Handling
- Formeln und Werte
```bash
pip install openpyxl
```

**langchain** als Orchestrierung f√ºr das RAG-System
- Text-Splitting
- Embedding-Integration
- Vector-Store-Anbindung
```bash
pip install langchain
```

**ChromaDB** als lokale Vektor-Datenbank
- Embedding-Speicherung
- Similarity-Search
- Metadaten-Filterung
```bash
pip install chromadb
```

### Code-Dokumentation

Der Code wird mit ausf√ºhrlichen **Docstrings** dokumentiert. Jede Funktion erkl√§rt ihre Parameter und R√ºckgabewerte. Beispiele zeigen die Verwendung.

**Beispiel:**
```python
def parse_document(file_path: Path) -> ParseResult:
    """
    Parst ein Dokument und extrahiert alle Informationen.
    
    Args:
        file_path: Pfad zur Datei (PDF, DOCX oder XLSX)
    
    Returns:
        ParseResult: Objekt mit volltext, struktur und metadaten
    
    Raises:
        ParsingError: Bei nicht unterst√ºtzten Formaten oder Parsing-Fehlern
    
    Example:
        >>> result = parse_document(Path("projektskizze.pdf"))
        >>> print(result.volltext[:100])
        "Projektname: Entwicklung einer..."
    """
    pass
```

---

## ‚ö° PERFORMANCE-OPTIMIERUNG

### Parallelverarbeitung
Mehrere Dokumente werden gleichzeitig geparst (multiprocessing).

```python
from multiprocessing import Pool

def parse_documents_parallel(file_paths: list[Path]) -> list[ParseResult]:
    """Parst mehrere Dokumente parallel."""
    with Pool(processes=4) as pool:
        results = pool.map(parse_document, file_paths)
    return results
```

### Caching
Bereits verarbeitete Dokumente werden markiert und nicht erneut geparst.

```python
def is_cached(file_path: Path) -> bool:
    """Pr√ºft, ob Dokument bereits geparst wurde."""
    cache_file = get_cache_path(file_path)
    return cache_file.exists()

def load_from_cache(file_path: Path) -> ParseResult:
    """L√§dt geparstes Ergebnis aus Cache."""
    cache_file = get_cache_path(file_path)
    with open(cache_file, "r", encoding="utf-8") as f:
        return ParseResult.from_json(f.read())
```

### Batch-Operationen
Embeddings werden in Gruppen generiert f√ºr bessere Effizienz.

```python
def generate_embeddings_batch(chunks: list[str], batch_size: int = 100):
    """Generiert Embeddings in Batches."""
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        embeddings = embedding_model.embed(batch)
        yield embeddings
```

---

## üìä ERWARTETES ERGEBNIS

Nach dem Parsing haben wir:

‚úÖ **Vollst√§ndigen, durchsuchbaren Text** aller Dokumente  
‚úÖ **Strukturierte Metadaten** f√ºr pr√§zise Filterung  
‚úÖ **Optimale Chunks** f√ºr RAG-Retrieval  
‚úÖ **Erweiterbare Codebasis** f√ºr zuk√ºnftige Anforderungen  

### Beispiel-Output

```json
{
  "dokument_id": "projektskizze_001",
  "dokumenttyp": "projektskizze",
  "volltext": "Kompletter Text der Projektskizze...",
  "struktur": {
    "kapitel": [...],
    "tabellen": [...]
  },
  "metadaten": {
    "dateiname": "projektskizze.pdf",
    "erstellt_am": "2025-03-15",
    "seiten": 3
  },
  "chunks": [
    {
      "chunk_id": "chunk_001",
      "text": "Chunk 1 Inhalt...",
      "metadata": {...}
    }
  ],
  "vector_ids": ["vec_001", "vec_002", ...]
}
```

---

## üéì ANWENDUNGSBEISPIEL

Das System erm√∂glicht es, Fragen wie:

**"Welche Voraussetzungen hat das PROFI-Programm?"**  
‚Üí RAG findet relevante Textstellen aus F√∂rderrichtlinien

**"Was steht in der Projektskizze zum Thema Innovation?"**  
‚Üí RAG extrahiert Abschnitt "Technologischer L√∂sungsansatz"

**"Ist das Unternehmen KMU-berechtigt?"**  
‚Üí RAG findet Informationen aus KMU-Erkl√§rung und Jahresabschl√ºssen

pr√§zise zu beantworten, indem es die relevanten Textstellen aus den geparsten Dokumenten findet und dem LLM zur Verf√ºgung stellt.

---

## üîê SICHERHEIT

### Datei-Validierung
- Dateityp-Pr√ºfung vor Verarbeitung
- Gr√∂√üenlimits (max. 50 MB pro Datei)
- Virus-Scan (optional integrierbar)

### Sandbox-Ausf√ºhrung
- Parser laufen in isolierter Umgebung
- Kein Zugriff auf Systemressourcen
- Timeout bei h√§ngenden Operationen

### Datenschutz
- Lokale Verarbeitung (keine Cloud)
- Verschl√ºsselte Speicherung m√∂glich
- DSGVO-konform

---

*Diese Spezifikation dient als Arbeitsgrundlage f√ºr die Implementierung des Dokumentenparsing-Moduls. Jeder Abschnitt kann in konkrete Entwicklungs-Tasks √ºbersetzt werden.*