# Technische Architektur
## IFB PROFI - KI-gestÃ¼tzte Textverarbeitung

**Version:** 3.0 (Architektur-Varianten)  
**Stand:** 10. November 2025  
**Zielgruppe:** Entwickler-Team

---

## ðŸŽ¯ ARCHITEKTUR-VARIANTEN

Das System kann in drei KomplexitÃ¤tsstufen implementiert werden:

### **Option 1: Super-Lite** (Empfohlen fÃ¼r schnellen Start)
- **Ziel:** FunktionsfÃ¤higer MVP in 1 Woche
- **LLM:** LM Studio (inkl. RAG-Features)
- **RAG:** LM Studio Built-in Collections
- **Hosting:** Komplett lokal
- **Aufwand:** Minimal

### **Option 2: Lite** (Mehr Kontrolle)
- **Ziel:** Produktionsreife in 2-3 Wochen
- **LLM:** LM Studio (nur Inferenz)
- **RAG:** Eigenes System (ChromaDB + LangChain)
- **Hosting:** Lokal/Hybrid
- **Aufwand:** Mittel

### **Option 3: Full** (Enterprise)
- **Ziel:** Skalierbare Cloud-LÃ¶sung
- **LLM:** Eigenes Hosting (vLLM/Ollama)
- **RAG:** Full-Stack (ChromaDB/Weaviate + Custom Pipeline)
- **Hosting:** Cloud/Kubernetes
- **Aufwand:** Hoch

---

## 1. SYSTEM-ÃœBERSICHT

## 1. SYSTEM-ÃœBERSICHT

### Option 1: Super-Lite Architektur (Empfohlen fÃ¼r MVP)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMLIT WEB-INTERFACE                      â”‚
â”‚                    (Wizard-basierte UI)                          â”‚
â”‚  â€¢ Projekt anlegen                                               â”‚
â”‚  â€¢ Dokumente hochladen                                           â”‚
â”‚  â€¢ Status-Tracking                                               â”‚
â”‚  â€¢ Reports & Checklisten                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PYTHON BACKEND (Minimal)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DOKUMENT-HANDLERâ”‚  LM STUDIO API   â”‚  KRITERIEN-ENGINE       â”‚
â”‚  â€¢ Upload        â”‚   CONNECTOR      â”‚  â€¢ Iterative PrÃ¼fung    â”‚
â”‚  â€¢ Speicherung   â”‚   â€¢ HTTP Client  â”‚  â€¢ Ergebnis-Sammlung    â”‚
â”‚  â€¢ Metadaten     â”‚   â€¢ Error Handle â”‚  â€¢ JSON-Speicherung     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LM STUDIO (All-in-One)                          â”‚
â”‚  â€¢ LLM Hosting (Qwen 2.5 3B-7B)                                 â”‚
â”‚  â€¢ RAG Built-in (Document Collections)                          â”‚
â”‚  â€¢ Embeddings (Integriert)                                      â”‚
â”‚  â€¢ OpenAI-kompatible API                                        â”‚
â”‚  â€¢ GUI fÃ¼r Modell-Management                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATENSPEICHERUNG                             â”‚
â”‚  â€¢ Lokales Dateisystem - Uploads & Projekte                     â”‚
â”‚  â€¢ JSON-Files - Metadaten & Ergebnisse                          â”‚
â”‚  â€¢ LM Studio Collections - RAG Dokumente                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Vorteile:**
- âœ… Schnellste Implementierung (1 Woche machbar)
- âœ… Minimale KomplexitÃ¤t
- âœ… Keine eigene RAG-Infrastruktur
- âœ… GUI fÃ¼r Nicht-Techniker
- âœ… Alles lokal, datenschutzkonform

**Nachteile:**
- âŒ AbhÃ¤ngig von LM Studio Features
- âŒ Weniger Kontrolle Ã¼ber RAG-Prozess
- âŒ Begrenzte Anpassbarkeit

---

### Option 2: Lite Architektur (Eigenes RAG)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMLIT WEB-INTERFACE                      â”‚
â”‚                    (Wizard-basierte UI)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PYTHON BACKEND (Erweitert)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DOKUMENT-PARSER â”‚   RAG-SYSTEM     â”‚  KRITERIEN-ENGINE       â”‚
â”‚  â€¢ PDF/DOCX/XLSX â”‚   â€¢ LangChain    â”‚  â€¢ Iterative PrÃ¼fung    â”‚
â”‚  â€¢ Chunking      â”‚   â€¢ ChromaDB     â”‚  â€¢ RAG-Integration       â”‚
â”‚  â€¢ Metadaten     â”‚   â€¢ Embeddings   â”‚  â€¢ Validierung          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LM STUDIO (Nur LLM)                             â”‚
â”‚  â€¢ LLM Hosting (Qwen 2.5 7B)                                    â”‚
â”‚  â€¢ OpenAI-kompatible API                                        â”‚
â”‚  â€¢ Fokus auf Inferenz                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATENSPEICHERUNG                             â”‚
â”‚  â€¢ ChromaDB - Vector Store (Embeddings)                         â”‚
â”‚  â€¢ Lokales Dateisystem - Projektdateien & Uploads               â”‚
â”‚  â€¢ JSON-Files - Projektmetadaten & Ergebnisse                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Vorteile:**
- âœ… Mehr Kontrolle Ã¼ber RAG
- âœ… Optimierbare Chunking-Strategie
- âœ… Eigene Metadaten-Verwaltung
- âœ… LLM weiterhin einfach (LM Studio)

**Nachteile:**
- âŒ Mehr Entwicklungsaufwand (2-3 Wochen)
- âŒ ChromaDB Setup & Wartung
- âŒ Eigene Embedding-Pipeline

---

### Option 3: Full Architektur (Enterprise)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  WEB-INTERFACE (React/Vue)                       â”‚
â”‚               (Multi-User, Authentifizierung)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API GATEWAY / LOAD BALANCER                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MICROSERVICES BACKEND                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PARSER SERVICE  â”‚   RAG SERVICE    â”‚  INFERENCE SERVICE      â”‚
â”‚  â€¢ Scale on      â”‚   â€¢ Weaviate/    â”‚  â€¢ vLLM/TGI Hosting     â”‚
â”‚    Demand        â”‚     Qdrant       â”‚  â€¢ Load Balancing       â”‚
â”‚  â€¢ Queue System  â”‚   â€¢ Custom       â”‚  â€¢ GPU Cluster          â”‚
â”‚                  â”‚     Embeddings   â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DISTRIBUTED STORAGE                             â”‚
â”‚  â€¢ Vector DB Cluster (Weaviate/Qdrant)                          â”‚
â”‚  â€¢ Object Storage (S3/MinIO)                                    â”‚
â”‚  â€¢ PostgreSQL - Metadaten                                       â”‚
â”‚  â€¢ Redis - Caching                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Vorteile:**
- âœ… Maximale Kontrolle
- âœ… Production-ready Skalierung
- âœ… Multi-User, Multi-Tenant
- âœ… High Availability

**Nachteile:**
- âŒ Hohe KomplexitÃ¤t
- âŒ Deutlich mehr Aufwand (Monate)
- âŒ Hardware-Anforderungen
- âŒ DevOps-Know-how erforderlich

---

## 2. TECH-STACK DETAILS

### 2.1 Variantenvergleich

| Komponente | Super-Lite | Lite | Full |
|------------|------------|------|------|
| **LLM Runtime** | LM Studio | LM Studio | vLLM/TGI |
| **LLM Modell** | Qwen 2.5 3B-7B | Qwen 2.5 7B | Qwen 2.5 14B+ |
| **RAG System** | LM Studio Built-in | LangChain + ChromaDB | Custom + Weaviate |
| **Vector DB** | LM Studio Collections | ChromaDB | Weaviate/Qdrant |
| **Embeddings** | LM Studio (automatisch) | HuggingFace Models | Custom Fine-tuned |
| **Frontend** | Streamlit | Streamlit | React/Vue |
| **Backend** | Python (Minimal) | Python + LangChain | FastAPI Microservices |
| **Deployment** | Lokal | Lokal/Docker | Kubernetes/Cloud |
| **Setup Zeit** | 1 Woche | 2-3 Wochen | 2-3 Monate |

---

### 2.2 Option 1: Super-Lite Setup

**Kernidee:** LM Studio Ã¼bernimmt RAG, LLM-Hosting und API. Python nur fÃ¼r Business-Logik.

#### Tech-Stack
| Komponente | Technologie | Version | Zweck |
|------------|-------------|---------|-------|
| **LLM Server** | LM Studio | Latest | All-in-One (LLM + RAG) |
| **LLM Modell** | Qwen 2.5 3B | Latest | Schnelle Inferenz |
| **Runtime** | Python | 3.11+ | Backend-Logik |
| **Frontend** | Streamlit | 1.28+ | Web-Interface |
| **HTTP Client** | requests/httpx | Latest | LM Studio API Calls |

#### LM Studio Configuration

```python
# config.yaml (Super-Lite)
llm:
  provider: "lm_studio"
  base_url: "http://localhost:1234/v1"
  model: "qwen2.5-3b-instruct"
  use_builtin_rag: true  # Wichtig!
  
rag:
  provider: "lm_studio"  # Keine eigene Implementierung
  collection_name: "ifb_documents"

backend:
  document_handler: "simple"  # Nur Upload + Speicherung
  criteria_engine: "iterative"  # Sequential processing
```

#### Workflow Super-Lite

1. **Upload:** Python speichert Dokumente in `/data/projects/{id}/uploads/`
2. **Indexierung:** Python sendet Dokumente via API an LM Studio
3. **RAG:** LM Studio indexiert in eigener Collection
4. **PrÃ¼fung:** Python sendet Kriterien-Prompts mit RAG-Anfragen
5. **Antwort:** LM Studio liefert kontextbasierte Antworten
6. **Speicherung:** Python speichert Ergebnisse als JSON

**Beispiel: LM Studio API Call mit RAG**

```python
import requests

def check_criterion_superlite(criterion_prompt: str, project_id: str):
    """Kriterium mit LM Studio Built-in RAG prÃ¼fen"""
    
    response = requests.post(
        "http://localhost:1234/v1/chat/completions",
        json={
            "model": "qwen2.5-3b-instruct",
            "messages": [
                {
                    "role": "system",
                    "content": "Du bist ein FÃ¶rderantrag-PrÃ¼fer fÃ¼r IFB Hamburg."
                },
                {
                    "role": "user",
                    "content": criterion_prompt
                }
            ],
            "temperature": 0.3,
            "max_tokens": 1000,
            # RAG-Aktivierung (LM Studio spezifisch)
            "collection": f"projekt_{project_id}",
            "use_rag": True,
            "top_k_chunks": 5
        }
    )
    
    return response.json()["choices"][0]["message"]["content"]
```

**Kritischer Punkt:** PrÃ¼fen, ob LM Studio diese RAG-Features bietet! Falls nicht â†’ Option 1.5 (siehe unten).

---

### 2.3 Option 2: Lite Setup

**Kernidee:** LM Studio nur fÃ¼r LLM. Eigenes RAG mit ChromaDB + LangChain.

#### Tech-Stack
| Komponente | Technologie | Version | Zweck |
|------------|-------------|---------|-------|
| **LLM Server** | LM Studio | Latest | LLM Inferenz |
| **LLM Modell** | Qwen 2.5 7B | Latest | Hauptmodell |
| **RAG Framework** | LangChain | 0.1+ | RAG-Pipeline |
| **Vector DB** | ChromaDB | 0.4.18+ | Embeddings-Speicher |
| **Embeddings** | HuggingFace | - | multilingual-e5-large |
| **Runtime** | Python | 3.11+ | Backend-Sprache |
| **Frontend** | Streamlit | 1.28+ | Web-Interface |

#### Configuration

```python
# config.yaml (Lite)
llm:
  provider: "lm_studio"
  base_url: "http://localhost:1234/v1"
  model: "qwen2.5-7b-instruct"
  
rag:
  provider: "chromadb"  # Eigenes System
  persist_directory: "./data/chromadb"
  embedding_model: "intfloat/multilingual-e5-large"
  chunk_size: 1000
  chunk_overlap: 200
  
backend:
  document_parser: "full"  # PDF/DOCX/XLSX Parser
  criteria_engine: "rag_enhanced"  # Mit eigenem RAG
```

#### RAG-System Setup

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from openai import OpenAI

# Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# Vector Store
vectorstore = Chroma(
    collection_name=f"projekt_{projekt_id}",
    embedding_function=embeddings,
    persist_directory="./data/chromadb"
)

# LLM Client (LM Studio)
llm_client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"
)

def check_criterion_lite(criterion_prompt: str, projekt_id: str):
    """Kriterium mit eigenem RAG prÃ¼fen"""
    
    # 1. Relevante Chunks via ChromaDB finden
    docs = vectorstore.similarity_search(
        criterion_prompt,
        k=5
    )
    
    # 2. Kontext zusammenstellen
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # 3. LLM-Anfrage mit Kontext
    response = llm_client.chat.completions.create(
        model="qwen2.5-7b-instruct",
        messages=[
            {
                "role": "system",
                "content": "Du bist ein FÃ¶rderantrag-PrÃ¼fer fÃ¼r IFB Hamburg."
            },
            {
                "role": "user",
                "content": f"Kontext:\n{context}\n\nAufgabe:\n{criterion_prompt}"
            }
        ],
        temperature=0.3,
        max_tokens=1000
    )
    
    return response.choices[0].message.content
```

---

### 2.4 Option 3: Full Setup

**Kernidee:** Komplette Eigenentwicklung mit Cloud-Readiness.

#### Tech-Stack
| Komponente | Technologie | Version | Zweck |
|------------|-------------|---------|-------|
| **LLM Runtime** | vLLM | Latest | Production LLM Serving |
| **LLM Modell** | Qwen 2.5 14B | Latest | GrÃ¶ÃŸeres Modell |
| **RAG Framework** | Custom | - | Optimierte Pipeline |
| **Vector DB** | Weaviate | Latest | Enterprise Vector DB |
| **Embeddings** | Custom Fine-tuned | - | Domain-spezifisch |
| **API Gateway** | FastAPI | Latest | Microservices |
| **Frontend** | React | 18+ | Modern Web UI |
| **Queue System** | Redis/RabbitMQ | Latest | Async Processing |
| **Database** | PostgreSQL | 15+ | Metadaten |
| **Deployment** | Kubernetes | 1.28+ | Orchestration |

**Hinweis:** Option 3 ist fÃ¼r dieses Projekt Ã¼berdimensioniert. Nur bei Multi-Tenant-Anforderungen sinnvoll.

---

### 2.5 Option 1.5: Super-Lite ohne LM Studio RAG

Falls LM Studio keine RAG-Features bietet, hier die Hybrid-LÃ¶sung:

**Kernidee:** LM Studio nur fÃ¼r LLM. Minimales RAG mit ChromaDB (vereinfacht).

```python
# Minimales RAG (kein LangChain!)
from chromadb import Client
from sentence_transformers import SentenceTransformer

# Simple Embedding Model
embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# ChromaDB (einfach)
chroma_client = Client()
collection = chroma_client.create_collection(f"projekt_{projekt_id}")

# Dokumente indexieren
def index_document(text: str, metadata: dict):
    chunks = simple_chunk(text, size=1000)  # Einfache Chunking-Funktion
    embeddings = embedder.encode(chunks)
    
    collection.add(
        documents=chunks,
        embeddings=embeddings.tolist(),
        metadatas=[metadata] * len(chunks),
        ids=[f"chunk_{i}" for i in range(len(chunks))]
    )

# RAG Retrieval
def retrieve_context(query: str, top_k=5):
    query_embedding = embedder.encode([query])
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=top_k
    )
    return results['documents'][0]
```

**Vorteil:** Immer noch sehr einfach, aber volle Kontrolle Ã¼ber RAG.

---

### 2.6 Dokumenten-Parser (Alle Varianten)

**Parser sind variantenunabhÃ¤ngig** - Alle drei Optionen nutzen dieselbe Parser-Infrastruktur.

**UnterstÃ¼tzte Formate:**

| Format | Library | Verwendung | KomplexitÃ¤t |
|--------|---------|------------|-------------|
| **PDF** | PyMuPDF (fitz) | Projektskizze, Gutachten | Mittel |
| **DOCX** | python-docx | Word-Dokumente, Vorlagen | Einfach |
| **XLSX** | openpyxl | Kalkulationen, FinanzÃ¼bersichten | Einfach |

**Hinweis:** Super-Lite kann mit vereinfachtem Parsing starten (nur Text), Lite/Full nutzen volle Features.

**Parser-Architektur:**

```python
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, Any

class BaseParser(ABC):
    """Abstract Base Class fÃ¼r alle Parser."""
    
    @abstractmethod
    def parse(self, file_path: Path) -> Dict[str, Any]:
        """
        Parst Datei und extrahiert strukturierte Daten.
        
        Returns:
            {
                "text": str,              # Volltext
                "metadata": dict,         # Titel, Datum, Autor
                "structured_data": dict,  # Strukturierte Felder
                "tables": list[dict]      # Extrahierte Tabellen (optional)
            }
        """
        pass
```

**Details:** Siehe `02_DOCUMENT_PARSING.md`

---

### 2.7 Datenspeicherung

**Speicher-Strategie nach Variante:**

| Komponente | Super-Lite | Lite | Full |
|------------|------------|------|------|
| **Vector Store** | LM Studio | ChromaDB | Weaviate/Qdrant |
| **Projektdateien** | Lokales FS | Lokales FS | S3/MinIO |
| **Metadaten** | JSON | JSON | PostgreSQL |
| **Caching** | - | - | Redis |

**Dateistruktur (Super-Lite & Lite):**

```
data/
â”œâ”€â”€ chromadb/                    # Vector Store (nur Lite)
â”‚   â””â”€â”€ chroma.sqlite3
â”‚
â”œâ”€â”€ projects/                    # Projektdaten
â”‚   â”œâ”€â”€ projekt_001/
â”‚   â”‚   â”œâ”€â”€ metadata.json       # Projekt-Metadaten
â”‚   â”‚   â”œâ”€â”€ uploads/            # Hochgeladene Dateien
â”‚   â”‚   â”‚   â”œâ”€â”€ projektskizze.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ kalkulation.xlsx
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ extracted/          # Geparste Daten
â”‚   â”‚   â”‚   â”œâ”€â”€ projektskizze.json
â”‚   â”‚   â”‚   â””â”€â”€ kalkulation.json
â”‚   â”‚   â””â”€â”€ results/            # PrÃ¼fungsergebnisse
â”‚   â”‚       â”œâ”€â”€ criteria_check.json
â”‚   â”‚       â””â”€â”€ report.md
â”‚   â””â”€â”€ projekt_002/
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ regelwerke/                  # FÃ¶rderrichtlinien (optional)
    â”œâ”€â”€ profi_foerderrichtlinie.pdf
    â””â”€â”€ bewertungskriterien.yaml
```

**Beispiel metadata.json:**

```json
{
  "projekt_id": "projekt_001",
  "projekt_name": "Vollautomatische Verpackungsmaschine",
  "antragsteller": "Verpackungsmaschinenbau GmbH",
  "modul": "PROFI Standard",
  "status": "in_review",
  "created_at": "2024-10-31T10:00:00Z",
  "updated_at": "2024-10-31T14:30:00Z",
  "architecture_variant": "super_lite",
  "documents": [
    {
      "doc_id": "doc_001",
      "doc_type": "projektskizze",
      "filename": "projektskizze.pdf",
      "uploaded_at": "2024-10-31T10:05:00Z",
      "parsed": true,
      "indexed_in_rag": true
    }
  ]
}
```

---

---

## 3. WIZARD-FLOW (7 Schritte)

### Schritt 1: Projekt anlegen
- Input: Projektname, Antragsteller, Modul
- Output: Neues Projekt in `data/projects/projekt_XXX/`

### Schritt 2: Dokumente hochladen
- Input: PDF, DOCX, XLSX-Dateien
- Output: Dateien in `uploads/` gespeichert

### Schritt 3: Dokumenten-Parsing
- Prozess: Parser fÃ¼r jeden Dateityp
- Output: JSON-Files in `extracted/`

### Schritt 4: Informationsextraktion (RAG)
- Prozess: 
  1. Text chunken
  2. Embeddings erstellen
  3. In ChromaDB speichern
  4. LLM-basierte Extraktion strukturierter Daten
- Output: Strukturierte Daten in `metadata.json`

### Schritt 5: FÃ¶rdervoraussetzungen prÃ¼fen
- Prozess: Use-Case-spezifische Checks via RAG + LLM
- Output: `foerdervoraussetzungen.json` + Checkliste

### Schritt 6: Bewertung
- Prozess: 5 Bewertungskriterien scoring
- Output: `bewertung.json`

### Schritt 7: Report generieren
- Output: Markdown-Report oder PDF

---

## 4. USE-CASE-SPEZIFISCHE CHECKS

**Ansatz:** Pro Dokumententyp definierte Checks

**Beispiel: KMU-ErklÃ¤rung prÃ¼fen**

```python
class KMUCheck:
    """PrÃ¼ft KMU-Status aus KMU-ErklÃ¤rung."""
    
    def __init__(self, rag_retriever, llm_client):
        self.retriever = rag_retriever
        self.llm = llm_client
    
    def check_mitarbeiter(self, projekt_id: str) -> dict:
        """PrÃ¼ft: Mitarbeiterzahl < 250."""
        
        # 1. RAG: Relevante Dokument-Chunks holen
        docs = self.retriever.retrieve(
            query="Mitarbeiterzahl Anzahl BeschÃ¤ftigte",
            filters={"projekt_id": projekt_id, "doc_type": "kmu_erklaerung"}
        )
        
        # 2. LLM: Extrahiere Mitarbeiterzahl
        prompt = f"""Extrahiere die Mitarbeiterzahl aus folgendem Text:

{docs[0].content}

Antworte nur mit einer Zahl, z.B.: 45"""
        
        response = self.llm.generate(prompt, temperature=0.1)
        mitarbeiter = int(response.strip())
        
        # 3. Check
        return {
            "kriterium": "Mitarbeiterzahl < 250",
            "wert": mitarbeiter,
            "erfuellt": mitarbeiter < 250,
            "begruendung": f"Das Unternehmen hat {mitarbeiter} Mitarbeiter."
        }
    
    def check_jahresumsatz(self, projekt_id: str) -> dict:
        """PrÃ¼ft: Jahresumsatz â‰¤ 50 Mio. EUR."""
        # Analog zu check_mitarbeiter
        pass
    
    def check_bilanzsumme(self, projekt_id: str) -> dict:
        """PrÃ¼ft: Bilanzsumme â‰¤ 43 Mio. EUR."""
        # Analog
        pass
    
    def run_all_checks(self, projekt_id: str) -> dict:
        """FÃ¼hrt alle KMU-Checks durch."""
        return {
            "mitarbeiter": self.check_mitarbeiter(projekt_id),
            "jahresumsatz": self.check_jahresumsatz(projekt_id),
            "bilanzsumme": self.check_bilanzsumme(projekt_id)
        }
```

**Checklisten-Output (Markdown):**

```markdown
# KMU-Status PrÃ¼fung

## Projektskizze: Verpackungsmaschinenbau GmbH

| Kriterium | Wert | Grenzwert | Status | BegrÃ¼ndung |
|-----------|------|-----------|--------|------------|
| Mitarbeiterzahl | 45 | < 250 | âœ… ErfÃ¼llt | Unternehmen hat 45 Mitarbeiter |
| Jahresumsatz | 8,5 Mio. â‚¬ | â‰¤ 50 Mio. â‚¬ | âœ… ErfÃ¼llt | Umsatz liegt unter Grenzwert |
| Bilanzsumme | 6,2 Mio. â‚¬ | â‰¤ 43 Mio. â‚¬ | âœ… ErfÃ¼llt | Bilanzsumme unter Grenzwert |

**Ergebnis:** KMU-Status bestÃ¤tigt âœ…
```

---

## 5. REGELWERK-ENGINE

**FÃ¶rdervoraussetzungen als YAML:**

```yaml
# data/regelwerke/foerdervoraussetzungen.yaml

foerdervoraussetzungen:
  - id: projektort
    name: "Projektort in Hamburg"
    typ: boolean
    bedingung: "BetriebsstÃ¤tte muss in Hamburg sein"
    quellen:
      - handelsregisterauszug
      - projektbeschreibung
    check_prompt: |
      PrÃ¼fe anhand der Dokumente: Hat das Unternehmen eine BetriebsstÃ¤tte in Hamburg?
      Antworte nur mit "Ja" oder "Nein" und einer kurzen BegrÃ¼ndung.
  
  - id: unternehmensalter
    name: "Etabliertes Unternehmen"
    typ: numeric
    bedingung: "GegrÃ¼ndet vor mindestens 3 Jahren"
    quellen:
      - handelsregisterauszug
    check_prompt: |
      Extrahiere das GrÃ¼ndungsjahr des Unternehmens.
      Berechne: Ist das Unternehmen mindestens 3 Jahre alt?
      Antworte im JSON-Format: {"gruendungsjahr": YYYY, "alter_jahre": X, "erfuellt": true/false}
  
  # ... weitere Voraussetzungen
```

**Check-Engine:**

```python
import yaml
from pathlib import Path

class FoerdervoraussetzungenEngine:
    """LÃ¤dt Regelwerk und fÃ¼hrt Checks durch."""
    
    def __init__(self, regelwerk_path: Path, rag_retriever, llm_client):
        with open(regelwerk_path) as f:
            self.regelwerk = yaml.safe_load(f)
        self.retriever = rag_retriever
        self.llm = llm_client
    
    def check_voraussetzung(self, voraussetzung_id: str, projekt_id: str) -> dict:
        """FÃ¼hrt Check fÃ¼r eine FÃ¶rdervoraussetzung durch."""
        
        # 1. Regelwerk laden
        regel = next(
            r for r in self.regelwerk["foerdervoraussetzungen"]
            if r["id"] == voraussetzung_id
        )
        
        # 2. Relevante Dokumente holen
        docs = self.retriever.retrieve(
            query=regel["name"],
            filters={
                "projekt_id": projekt_id,
                "doc_type": regel["quellen"]
            }
        )
        
        # 3. LLM-Check
        context = "\n\n".join([d.content for d in docs[:3]])
        prompt = f"{regel['check_prompt']}\n\nKontext:\n{context}"
        
        response = self.llm.generate(prompt, temperature=0.1)
        
        # 4. Ergebnis parsen und zurÃ¼ckgeben
        return {
            "voraussetzung": regel["name"],
            "erfuellt": "ja" in response.lower() or "true" in response.lower(),
            "antwort": response,
            "quellen": [d.source for d in docs]
        }
```

---

## 6. ERWEITERUNGEN (Optional)

### 6.1 MCP-Server-Integration

Falls ihr MCP (Model Context Protocol) nutzen wollt:

```python
# Beispiel: MCP-Server fÃ¼r Datenbankzugriff

from mcp import MCPServer

mcp_server = MCPServer("ifb-database")

@mcp_server.tool()
def get_projekt_info(projekt_id: str) -> dict:
    """Holt Projektinformationen aus dem Dateisystem."""
    metadata_path = f"data/projects/{projekt_id}/metadata.json"
    with open(metadata_path) as f:
        return json.load(f)

# In LangChain einbinden
from langchain.tools import Tool

tools = [
    Tool(
        name="get_projekt_info",
        func=mcp_server.get_tool("get_projekt_info"),
        description="Holt Projektinformationen"
    )
]
```

### 6.2 Visualisierungen (Plotly)

```python
import plotly.graph_objects as go

def create_bewertung_chart(bewertung: dict) -> go.Figure:
    """Erstellt Radar-Chart fÃ¼r Bewertungskriterien."""
    
    categories = [
        "Produktidee",
        "Innovationsgrad",
        "Team",
        "Vermarktung",
        "Arbeitsplatz/Umwelt"
    ]
    
    values = [
        bewertung["produktidee"]["score"],
        bewertung["innovationsgrad"]["score"],
        bewertung["team"]["score"],
        bewertung["vermarktung"]["score"],
        bewertung["arbeitsplatz_umwelt"]["score"]
    ]
    
    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=values,
        theta=categories,
        fill='toself',
        name='Bewertung'
    ))
    
    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
        showlegend=False,
        title="Bewertungsprofil"
    )
    
    return fig
```

---

## 7. DEPLOYMENT

### 7.1 Lokale Entwicklung

```bash
# 1. Repository klonen
git clone <your-repo>
cd ifb-profi-ki

# 2. Virtual Environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Dependencies
pip install -r requirements.txt

# 4. LM Studio starten (GUI oder CLI)
# - GUI: LM Studio Ã¶ffnen â†’ Modell laden â†’ Server starten
# - CLI: lms server start

# 5. ChromaDB initialisieren
python scripts/init_chromadb.py

# 6. Streamlit starten
streamlit run frontend/streamlit_app.py
```

### 7.2 Requirements.txt

```txt
# Core
python>=3.11

# LLM & RAG
langchain==0.1.0
chromadb==0.4.18
sentence-transformers==2.2.2
openai==1.3.0  # FÃ¼r LM Studio API (OpenAI-kompatibel)

# Document Parsing
PyMuPDF==1.23.8
python-docx==1.1.0
openpyxl==3.1.2

# Frontend
streamlit==1.28.2
plotly==5.18.0
streamlit-aggrid==0.3.4

# Data Validation
pydantic==2.5.2
pyyaml==6.0.1

# Utilities
python-dotenv==1.0.0
loguru==0.7.2
requests==2.31.0

# Testing
pytest==7.4.3
pytest-cov==4.1.0
```

---

## 8. PROJEKTSTRUKTUR

```
ifb-profi-ki/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_parser.py
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py
â”‚   â”‚   â”œâ”€â”€ docx_parser.py
â”‚   â”‚   â””â”€â”€ xlsx_parser.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â””â”€â”€ lm_studio_client.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rules/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ foerdervoraussetzungen.py
â”‚   â”‚   â”œâ”€â”€ kmu_check.py
â”‚   â”‚   â””â”€â”€ bewertung.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ 1_Projekt_anlegen.py
â”‚       â”œâ”€â”€ 2_Dokumente_hochladen.py
â”‚       â”œâ”€â”€ 3_Parsing.py
â”‚       â”œâ”€â”€ 4_Extraktion.py
â”‚       â”œâ”€â”€ 5_Foerdervoraussetzungen.py
â”‚       â”œâ”€â”€ 6_Bewertung.py
â”‚       â””â”€â”€ 7_Report.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chromadb/
â”‚   â”œâ”€â”€ projects/
â”‚   â””â”€â”€ regelwerke/
â”‚       â”œâ”€â”€ foerdervoraussetzungen.yaml
â”‚       â””â”€â”€ profi_foerderrichtlinie.pdf
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_parsers.py
â”‚   â”œâ”€â”€ test_rag.py
â”‚   â””â”€â”€ test_rules.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_chromadb.py
â”‚   â””â”€â”€ setup.sh
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml
```

---

## 9. PERFORMANCE & HARDWARE

### 9.1 Getestet auf M1 Mac

**Hardware:**
- MacBook Pro M1
- 16 GB RAM
- macOS Sonoma

**Modell:** Qwen 2.5 3B Instruct

**Performance:**
- Parsing: ~2-3 Sek/Dokument
- Embedding: ~1 Sek/1000 Tokens
- LLM-Inferenz: ~20-30 Tokens/Sek
- Gesamtdurchlauf (1 Projekt): ~2-3 Minuten

### 9.2 Empfehlungen

| Hardware | Empfohlenes Modell | Performance |
|----------|-------------------|-------------|
| M1/M2 Mac (8-16GB) | Qwen 2.5 3B | Gut |
| M1/M2 Mac (16-32GB) | Qwen 2.5 7B | Sehr gut |
| Windows/Linux (16GB RAM) | Qwen 2.5 3B | Gut |
| Windows/Linux (32GB RAM + GPU) | Qwen 2.5 7B | Sehr gut |

---

## 10. NÃ„CHSTE SCHRITTE

### Phase 1: Setup (diese Woche)
- [ ] Git-Repo erstellen
- [ ] Projektstruktur aufbauen
---

## 3. EMPFEHLUNG & ENTSCHEIDUNGSHILFE

### FÃ¼r dieses Projekt: **Option 1.5 (Super-Lite mit minimalem RAG)**

**BegrÃ¼ndung:**
1. **LM Studio RAG-Features unsicher** - Nicht alle Versionen bieten vollwertige RAG-APIs
2. **Volle Kontrolle Ã¼ber RAG** - Kriterienkatalog benÃ¶tigt prÃ¤zise Chunk-Auswahl
3. **Schnell umsetzbar** - Minimales ChromaDB + sentence-transformers (keine LangChain)
4. **Einfach wartbar** - Weniger Dependencies, klarer Code
5. **Upgrade-fÃ¤hig** - SpÃ¤ter einfach zu Lite/Full erweiterbar

### Konkrete Stack-Empfehlung

```yaml
# config/system_config.yaml (Empfohlen)

llm:
  provider: "lm_studio"
  base_url: "http://localhost:1234/v1"
  model: "qwen2.5-7b-instruct"  # 7B fÃ¼r bessere QualitÃ¤t
  temperature: 0.3
  max_tokens: 2000

rag:
  provider: "chromadb"
  persist_directory: "./data/chromadb"
  embedding_model: "paraphrase-multilingual-MiniLM-L12-v2"  # Kompakt & schnell
  chunk_size: 1000
  chunk_overlap: 200
  top_k: 5

parser:
  pdf: "pymupdf"
  docx: "python-docx"
  xlsx: "openpyxl"
  
storage:
  projects_dir: "./data/projects"
  uploads_subdir: "uploads"
  extracted_subdir: "extracted"
  results_subdir: "results"
```

### Minimale Dependencies (requirements.txt)

```txt
# LLM & RAG
openai==1.3.0              # OpenAI-Client fÃ¼r LM Studio API
chromadb==0.4.18           # Vector Database
sentence-transformers==2.2.2  # Embeddings (kein HuggingFace overhead)

# Document Parsing
pymupdf==1.23.8            # PDF
python-docx==1.1.0         # DOCX
openpyxl==3.1.2            # XLSX

# Frontend
streamlit==1.28.0          # UI

# Utils
pydantic==2.5.0            # Validierung
python-dotenv==1.0.0       # Config
```

**GeschÃ¤tzte Entwicklungszeit:** 5-7 Tage fÃ¼r MVP

---

## 4. IMPLEMENTIERUNGS-ROADMAP

### Phase 1: Fundament (Tag 1-2)

**Ziel:** Basis-Setup funktionsfÃ¤hig

- [x] Projektstruktur anlegen
- [ ] Config-System (`config/system_config.yaml`)
- [ ] LM Studio installieren & testen
- [ ] Python Environment & Dependencies
- [ ] Minimale Streamlit-App (Hello World)

**Testkriterium:** LM Studio antwortet auf API-Call

### Phase 2: Dokumenten-Upload & Parsing (Tag 2-3)

**Ziel:** Dokumente hochladen und parsen

- [ ] Streamlit Upload-Komponente
- [ ] PDF-Parser (nur Text-Extraktion)
- [ ] Speicherung in `/data/projects/{id}/uploads/`
- [ ] JSON-Export des geparsten Texts

**Testkriterium:** PDF hochladen â†’ Text extrahiert â†’ JSON gespeichert

### Phase 3: Minimales RAG-System (Tag 3-4)

**Ziel:** Dokumente indexieren und suchen

- [ ] ChromaDB Setup & Initialisierung
- [ ] Embedding-Model laden (sentence-transformers)
- [ ] Simple Chunking-Funktion
- [ ] Indexierungs-Pipeline
- [ ] Test: Dokument indexieren â†’ Similarity Search funktioniert

**Testkriterium:** Query "Hamburg Standort" findet relevante Chunks

### Phase 4: LLM-Integration & Kriterien-Engine (Tag 4-5)

**Ziel:** Erste Kriterien-PrÃ¼fung automatisiert

- [ ] LM Studio API-Client (OpenAI-kompatibel)
- [ ] Kriterien-Katalog laden (`config/criteria_catalog.json`)
- [ ] Iterative PrÃ¼fung (ein Kriterium nach dem anderen)
- [ ] RAG + LLM kombinieren
- [ ] Ergebnis als JSON speichern

**Testkriterium:** Kriterium "Projektort" wird korrekt geprÃ¼ft

### Phase 5: VollstÃ¤ndiger Wizard (Tag 5-6)

**Ziel:** Kompletter User-Flow

- [ ] Seite 1: Projekt anlegen
- [ ] Seite 2: Dokumente hochladen
- [ ] Seite 3: Automatische PrÃ¼fung (mit Progress)
- [ ] Seite 4: ErgebnisÃ¼bersicht
- [ ] Navigation zwischen Seiten

**Testkriterium:** VollstÃ¤ndiger Durchlauf von Projekt-Anlage bis Ergebnis

### Phase 6: Polishing & Reports (Tag 6-7)

**Ziel:** Production-ready MVP

- [ ] Error-Handling verbessern
- [ ] Loading-States & Progress-Bars
- [ ] Export-Funktionen (JSON, Markdown)
- [ ] Terminal-Logging (siehe DEVELOPMENT_PRINCIPLES.md)
- [ ] Dokumentation vervollstÃ¤ndigen

**Testkriterium:** Demo mit echten IFB-Dokumenten lÃ¤uft durch

---

## 5. MIGRATIONS-PFADE

### Von Super-Lite zu Lite

**Ã„nderungen:**
1. LangChain installieren
2. RAG-Code auf LangChain-Abstractions umstellen
3. Bessere Chunking-Strategie (RecursiveCharacterTextSplitter)
4. Eigene Embedding-Pipeline

**Aufwand:** 2-3 Tage

### Von Lite zu Full

**Ã„nderungen:**
1. Backend zu FastAPI umbauen
2. LM Studio â†’ vLLM/TGI
3. ChromaDB â†’ Weaviate/Qdrant
4. Streamlit â†’ React Frontend
5. Docker/Kubernetes Setup

**Aufwand:** 3-4 Wochen

**Empfehlung:** Nur bei echten Production-Anforderungen (Multi-User, Skalierung)

---

**Ende der Technischen Architektur**

**NÃ¤chste Schritte:**
- Siehe `SYSTEM_REQUIREMENTS.md` fÃ¼r Hardware-Details
- Siehe `03_RAG_SYSTEM.md` fÃ¼r RAG-Implementierung
- Siehe `04_LLM_INTEGRATION.md` fÃ¼r LLM-Setup

Bei Fragen: Siehe README.md oder kontaktiert das Team!

