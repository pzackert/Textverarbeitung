# System Requirements

**Version:** 2.0 (Architektur-Varianten)  
**Stand:** 10. November 2025

---

## üéØ √úBERSICHT

Systemanforderungen variieren je nach gew√§hlter Architektur-Option:
- **Super-Lite:** Lokales Development/Testing
- **Lite:** Production Single-User
- **Full:** Enterprise Multi-User Cloud

---

## üîß Option 1: Super-Lite Requirements

### Hardware (Empfohlen f√ºr MVP)
- **CPU:** Apple M1/M2 oder Intel i5/AMD Ryzen 5 (4+ Kerne)
- **RAM:** 16GB minimum, 24GB empfohlen
- **Storage:** 20GB SSD (10GB f√ºr LM Studio + Modelle, 10GB f√ºr Daten)
- **GPU:** Optional (Metal auf Mac, CUDA auf Windows/Linux beschleunigt)

### Software
- **OS:** macOS 11+, Windows 10/11, Linux (Ubuntu 20.04+)
- **Python:** 3.11+ (mit pip)
- **LM Studio:** Latest (Download: https://lmstudio.ai)
- **Git:** F√ºr Version Control

### LM Studio Modelle
- **Qwen 2.5 3B Instruct** (3-4 GB) - Schnell, ausreichend f√ºr Tests
- **Qwen 2.5 7B Instruct** (6-7 GB) - Bessere Qualit√§t, empfohlen

### Dependencies (Python)
```txt
openai==1.3.0
chromadb==0.4.18
sentence-transformers==2.2.2
pymupdf==1.23.8
python-docx==1.1.0
openpyxl==3.1.2
streamlit==1.28.0
pydantic==2.5.0
```

**Gesamtgr√∂√üe:** ~5GB (Python Packages + Embedding Model)

### Network
- **Lokal:** Kein Internet nach Setup ben√∂tigt
- **Ports:** 1234 (LM Studio), 8501 (Streamlit)

### Performance-Erwartungen
- **Dokument-Indexierung:** 2-5 Sekunden (10 Seiten PDF)
- **Kriterien-Pr√ºfung:** 5-10 Sekunden pro Kriterium
- **Gesamt-Pr√ºfung:** 30-60 Sekunden (6 Kriterien)
- **Gleichzeitige User:** 1

### Features
- ‚úÖ Single-User-Modus
- ‚úÖ Lokale Dokumenten-Verarbeitung
- ‚úÖ Basic RAG mit ChromaDB
- ‚úÖ Streamlit UI
- ‚úÖ Dokumentgr√∂√üe: Bis 30MB
- ‚úÖ 100% Datenschutz (alles lokal)

### Einschr√§nkungen
- ‚ùå Kein Multi-User
- ‚ùå Keine Cloud-Integration
- ‚ùå Begrenzte Skalierung
- ‚ùå Einfaches Error-Handling

---

## üöÄ Option 2: Lite Requirements

### Hardware (Production Single/Small Team)
- **CPU:** Intel i7/AMD Ryzen 7 (6+ Kerne)
- **RAM:** 32GB empfohlen (24GB minimum)
- **Storage:** 50GB SSD
- **GPU:** 8GB VRAM empfohlen (NVIDIA RTX 3060 oder besser)

### Software
- **OS:** Linux (Ubuntu 22.04) empfohlen, macOS/Windows m√∂glich
- **Python:** 3.11+
- **LM Studio:** Latest
- **Docker:** Optional (f√ºr Container-Deployment)
- **Git:** F√ºr Version Control

### LM Studio Modelle
- **Qwen 2.5 7B Instruct** (6-7 GB) - Standard
- **Qwen 2.5 14B Instruct** (12-14 GB) - Optional, wenn genug VRAM

### Dependencies (Python)
```txt
# Alle aus Super-Lite +
langchain==0.1.0
langchain-community==0.1.0
```

**Gesamtgr√∂√üe:** ~7GB

### Network
- **Internet:** F√ºr Updates & Monitoring
- **Lokales Netzwerk:** F√ºr Team-Zugriff
- **Ports:** 1234 (LM Studio), 8501 (Streamlit), 8000 (FastAPI optional)

### Performance-Erwartungen
- **Dokument-Indexierung:** 1-3 Sekunden (10 Seiten PDF)
- **Kriterien-Pr√ºfung:** 3-5 Sekunden pro Kriterium (GPU)
- **Gesamt-Pr√ºfung:** 20-30 Sekunden (6 Kriterien)
- **Gleichzeitige User:** 1-5

### Features
- ‚úÖ Alle aus Super-Lite +
- ‚úÖ Erweiterte RAG-Pipeline (LangChain)
- ‚úÖ Bessere Chunking-Strategie
- ‚úÖ Optimierte Embeddings
- ‚úÖ Dokumentgr√∂√üe: Bis 100MB
- ‚úÖ Basic API-Endpunkte
- ‚úÖ Monitoring & Logging

### Optional (Lite+)
- Docker-Deployment
- Reverse Proxy (Nginx)
- Redis Cache
- PostgreSQL f√ºr Metadaten

---

## ‚≠ê Option 3: Full Requirements (Enterprise)

### Hardware (Cloud/On-Premise Cluster)
- **CPU:** Multi-Core Server (16+ Kerne)
- **RAM:** 64GB+ (128GB empfohlen)
- **Storage:** 500GB+ SSD (NVMe empfohlen)
- **GPU:** 
  - Development: NVIDIA A10/A100 (24GB+ VRAM)
  - Production: Multi-GPU Setup (A100 80GB empfohlen)

### Software Stack
- **OS:** Linux (Ubuntu 22.04 LTS)
- **Container:** Docker + Kubernetes
- **LLM Runtime:** vLLM oder Text-Generation-Inference (TGI)
- **Vector DB:** Weaviate oder Qdrant (Cluster-Mode)
- **API Gateway:** Kong oder Traefik
- **Message Queue:** RabbitMQ oder Kafka
- **Database:** PostgreSQL 15+ (Replicated)
- **Cache:** Redis Cluster
- **Monitoring:** Prometheus + Grafana
- **Logging:** ELK Stack

### LLM Models
- **Qwen 2.5 14B+** oder gr√∂√üer
- **Custom Fine-Tuned Models** (optional)
- **Multi-Model Support** (verschiedene Modelle f√ºr verschiedene Tasks)

### Cloud Provider (Beispiel AWS)
- **EC2:** p4d.24xlarge (8x A100 80GB) oder √§hnlich
- **S3:** F√ºr Dokument-Speicherung
- **RDS:** Managed PostgreSQL
- **ElastiCache:** Managed Redis
- **EKS:** Kubernetes Cluster
- **ALB:** Load Balancing

### Network
- **High-Speed Internet:** Gigabit+
- **VPN:** F√ºr sichere Zugriffe
- **CDN:** Optional f√ºr Frontend
- **API Gateway:** Rate Limiting, Auth

### Performance-Erwartungen
- **Dokument-Indexierung:** <1 Sekunde (10 Seiten PDF)
- **Kriterien-Pr√ºfung:** 1-2 Sekunden pro Kriterium
- **Gesamt-Pr√ºfung:** 10-15 Sekunden (6 Kriterien)
- **Gleichzeitige User:** Unbegrenzt (Auto-Scaling)
- **Throughput:** 100+ Anfragen/Sekunde

### Features
- ‚úÖ Enterprise Multi-Tenant
- ‚úÖ Unlimited Users
- ‚úÖ Advanced RAG mit Custom Embeddings
- ‚úÖ Multiple LLM Models
- ‚úÖ REST & GraphQL APIs
- ‚úÖ Real-time Collaboration
- ‚úÖ Document Version Control
- ‚úÖ Advanced Analytics
- ‚úÖ Dokumentgr√∂√üe: 500MB+
- ‚úÖ Automated Backups
- ‚úÖ Disaster Recovery
- ‚úÖ GDPR/SOC2 Compliance
- ‚úÖ CI/CD Pipeline

### DevOps Requirements
- Kubernetes Knowledge
- Infrastructure as Code (Terraform)
- Monitoring & Alerting Setup
- Security Hardening
- Load Testing
- Disaster Recovery Plans

---

## üìä Vergleichstabelle

| Feature | Super-Lite | Lite | Full |
|---------|------------|------|------|
| **Setup-Zeit** | 1 Tag | 3-5 Tage | 2-3 Monate |
| **Kosten (Hardware)** | ‚Ç¨1.500 Laptop | ‚Ç¨3.000 Workstation | ‚Ç¨50.000+ Server |
| **Kosten (Cloud/Monat)** | ‚Ç¨0 | ‚Ç¨0-100 | ‚Ç¨2.000-10.000+ |
| **Users** | 1 | 1-5 | Unbegrenzt |
| **Dokument-Gr√∂√üe** | 30MB | 100MB | 500MB+ |
| **Verarbeitungszeit** | 30-60s | 20-30s | 10-15s |
| **Skalierbarkeit** | Keine | Begrenzt | Auto-Scaling |
| **Wartung** | Minimal | Mittel | Hoch |
| **Internet n√∂tig** | Nein | Teilweise | Ja |
| **Deployment** | Lokal | Lokal/Docker | K8s/Cloud |

---

## üéØ EMPFEHLUNG F√úR IFB-PROJEKT

**Starten mit: Option 1 (Super-Lite)**

**Begr√ºndung:**
1. ‚úÖ Schnellster Start (MVP in 1 Woche)
2. ‚úÖ Minimale Kosten (vorhandene Hardware)
3. ‚úÖ 100% Datenschutz (lokal)
4. ‚úÖ Ausreichend f√ºr Proof-of-Concept
5. ‚úÖ Einfach zu warten

**Upgrade-Pfad:**
- **Nach MVP:** Evaluation mit echten IFB-Dokumenten
- **Bei Bedarf:** Upgrade zu Lite f√ºr bessere Performance
- **Nur wenn n√∂tig:** Full f√ºr Multi-User-Szenarien

**Kritische Entscheidungspunkte:**
- Mehr als 5 User? ‚Üí Lite oder Full
- Cloud-Integration erforderlich? ‚Üí Full
- Budget-Beschr√§nkungen? ‚Üí Super-Lite
- Proof-of-Concept? ‚Üí Super-Lite

---

## üîß INSTALLATIONS-GUIDE (Super-Lite)

### 1. System vorbereiten
```bash
# Python 3.11+ installieren
python --version  # Sollte 3.11+ sein

# Virtual Environment erstellen
python -m venv venv
source venv/bin/activate  # macOS/Linux
# oder: venv\Scripts\activate  # Windows
```

### 2. LM Studio installieren
1. Download: https://lmstudio.ai
2. Installieren & starten
3. Modell laden: Qwen 2.5 7B Instruct
4. Server starten (Port 1234)

### 3. Dependencies installieren
```bash
pip install -r requirements.txt
```

### 4. Projekt-Setup
```bash
# Ordnerstruktur erstellen
mkdir -p data/{chromadb,projects,regelwerke}

# Config kopieren
cp config/config.example.yaml config/config.yaml
```

### 5. Testen
```bash
# Streamlit starten
streamlit run frontend/app.py

# Browser √∂ffnet automatisch: http://localhost:8501
```

**Erwartete Ausgabe:**
```
[SYSTEM] IFB PROFI System gestartet
[LLM] ‚úì Verbunden mit LM Studio (localhost:1234)
[CHROMADB] ‚úì Vector Store bereit
[SYSTEM] ‚úì System bereit!
```

---

**Ende System Requirements**

**Siehe auch:**
- `TECHNICAL_ARCHITECTURE.md` f√ºr Architektur-Details
- `DEVELOPMENT_PRINCIPLES.md` f√ºr Best Practices
