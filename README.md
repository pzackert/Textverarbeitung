# IFB PROFI - KI-gestÃ¼tzte AntragsprÃ¼fung

**Automatisierte Validierung von FÃ¶rderantrÃ¤gen** mit lokalem KI-System und Kriterienkatalog.

## ğŸ¯ ProjektÃ¼bersicht

Diese Anwendung ermÃ¶glicht die **strukturierte PrÃ¼fung von FÃ¶rderantrÃ¤gen** durch:
- Automatische Datenextraktion aus verschiedenen Dokumentformaten
- Validierung anhand eines definierten Kriterienkatalogs
- KI-gestÃ¼tzte PlausibilitÃ¤tsprÃ¼fung und Bewertung
- Ãœbersichtliche Darstellung der PrÃ¼fergebnisse

### Workflow
**GefÃ¼hrter Wizard-Flow (Streamlit):**
1. Projekt im Dashboard anlegen (Hero + Sidebar-Suche)
2. Dokumentkarten mit Kriterien-Beschreibung aus `config/criteria_catalog.json` befÃ¼llen
3. Automatische PrÃ¼fung (Parsing â†’ RAG â†’ Kriterienengine) inkl. Fortschrittsbalken
4. Ergebnisse tabellarisch auswerten & JSON exportieren

## âœ¨ Features

- ğŸ“Š **Projekt-Management:** Ãœbersicht aller PrÃ¼fprojekte inkl. Sidebar-Suche & Status-Badges
- ğŸ› **Wizard mit Fortschrittsbalken:** Permanent sichtbare Steps (Metadaten â†’ Upload â†’ PrÃ¼fung â†’ Ergebnisse)
- ğŸ“„ **Dokumentkarten mit Kontext:** Jede Upload-Kachel zeigt Beschreibung & Kriterien aus dem Katalog
- ğŸ¤– **Lokales LLM:** LM Studio oder andere OpenAI-kompatible Server (private Cloud mÃ¶glich)
- ğŸ” **RAG-System:** ChromaDB + sentence-transformers fÃ¼r kontextbasierte Analyse
- âš™ï¸ **Regelwerk-Engine:** Automatische PrÃ¼fung gegen FÃ¶rdervoraussetzungen
- âœ… **Demo-Projekt:** Seeder legt ein vorfÃ¼hrbares Referenzprojekt automatisch an
- ğŸ”’ **Datenschutz:** 100% lokal, keine externen Cloud-Dienste

## ğŸ“ Projektstruktur

```
masterprojekt/
â”œâ”€â”€ backend/          # Core-Logik
â”‚   â”œâ”€â”€ parsers/      # PDF, DOCX, XLSX Parser
â”‚   â”œâ”€â”€ rag/          # ChromaDB, Chunking, Embeddings
â”‚   â”œâ”€â”€ llm/          # LM Studio Client
â”‚   â”œâ”€â”€ core/         # Criteria Engine
â”‚   â””â”€â”€ utils/        # Config, Logger
â”œâ”€â”€ frontend/         # Streamlit UI
â”‚   â”œâ”€â”€ app.py        # Wizard (Sidebar + Progress)
â”‚   â”œâ”€â”€ components/   # Sidebar, Progress Tracker & Cards
â”‚   â”œâ”€â”€ services/     # Project-/Process-Services (Backend Calls)
â”‚   â”œâ”€â”€ styles/       # IFB Copalette CSS
â”‚   â””â”€â”€ pages/        # Legacy Seiten (optional)
â”œâ”€â”€ config/           # YAML-Konfiguration
â”œâ”€â”€ data/             # Projekte, ChromaDB, Input
â”œâ”€â”€ docs/             # Detaillierte Dokumentation
â””â”€â”€ tests/            # Unit & Integration Tests
```

## ğŸ›  Tech-Stack

### Backend
- **Python:** 3.11+
- **Parser:** PyMuPDF, python-docx, openpyxl
- **RAG:** ChromaDB, sentence-transformers
- **LLM:** OpenAI-kompatible API (LM Studio, Ollama, etc.)

### Frontend
- **Streamlit:** Webbasierte UI

### LLM-Server
Verschiedene Optionen mÃ¶glich (in Evaluation):
- LM Studio (lokal)
- Ollama (lokal)
- Private Cloud-Deployment
- Modell: Qwen, Llama, Mistral (je nach Anforderung)

## ğŸš€ Installation & Setup

### Voraussetzungen
- Python 3.11+
- UV Package Manager oder venv
- LM Studio oder alternativer LLM-Server

### Installation

**Mit UV (empfohlen):**
```bash
# 1. Repository klonen
git clone <repo-url>
cd masterprojekt

# 2. Dependencies installieren
uv sync

# 3. Config anpassen
cp config/config.example.yaml config/config.yaml
# LLM-Server URL in config.yaml eintragen

# 4. Anwendung starten
python frontend/start.py
```

**Mit venv (alternativ):**
```bash
# 1. Repository klonen
git clone <repo-url>
cd masterprojekt

# 2. Virtual Environment erstellen
python3 -m venv venv
source venv/bin/activate

# 3. Dependencies installieren
pip install -e .

# 4. Config anpassen
cp config/config.example.yaml config/config.yaml

# 5. Anwendung starten
python frontend/start.py
```

### Anwendung starten

```bash
# Streamlit UI mit Live-Logs starten
cd frontend
python start.py

# In zweitem Terminal weiterarbeiten oder Copilot verwenden

# Zum Beenden (neues Terminal)
python stop.py
```

- Die Start-Routine nutzt das virtuelle Environment unter `venv/` und Ã¶ffnet den Browser automatisch.
- Live-Logs erscheinen direkt im Terminal; zum Beenden `Strg+C` oder `python stop.py` verwenden.
- App lÃ¤uft auf: **http://localhost:8501**

## ğŸ“š Dokumentation

Detaillierte Dokumentationen im `docs/` Ordner:
- **01-08:** Komponenten-spezifische Dokumentation (UI, Parsing, RAG, LLM, etc.)
- **GETTING_STARTED.md:** Schnellstart-Anleitung
- **PROJECT_OVERVIEW.md:** Architektur-Ãœbersicht
- **TECHNICAL_REQUIREMENTS.md:** System & Tech-Anforderungen

## ğŸ” Datenschutz & Sicherheit

- **Lokal-First:** Alle Daten bleiben auf lokalem System oder privater Cloud
- **Keine externen APIs:** LLM lÃ¤uft komplett lokal
- **Dateibasiert:** Projekte in `data/projects/`, keine externe Datenbank
- **Single-User:** MVP fÃ¼r Einzelnutzung (Multi-User in zukÃ¼nftigen Versionen)

## ğŸ§ª Tests

```bash
# Unit Tests
uv run pytest tests/unit/

# Integration Tests
uv run pytest tests/integration/
```

## ğŸ“ Lizenz

[Lizenz hier einfÃ¼gen]

---

**Version:** 1.0 (Option 1 MVP)  
**Stand:** November 2025
